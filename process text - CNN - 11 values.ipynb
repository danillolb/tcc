{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSANDO AS QUESTÕES\n",
    "#### Utilizando GloVe 100 Dim - Português\n",
    "#### Dataset: Questão sobre cientificidade da Psicologia - PUC GO - Prof. Weber Martins, PhD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danil\\appdata\\local\\conda\\conda\\envs\\tensorflowgpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "import ml_metrics as metrics\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove_s100')\n",
    "FNAME = 'preprocessado_sem_stopwords.csv'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indexando word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 934963 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "aux = 0 # a ideia do aux é pq a primeira linha do arquivo precisa ser descartada \n",
    "with open(os.path.join(GLOVE_DIR, 'glove_s100.txt'), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        if aux > 0:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        aux = 1\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "df = pd.read_csv(FNAME, encoding = \"iso-8859-1\")\n",
    "texts = df['resposta'].values.tolist()\n",
    "labels = df['nota'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1338 unique tokens.\n",
      "Shape of data tensor: (242, 100)\n",
      "Shape of label tensor: (242, 11)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) #instancia Tokenizer e num_words manterá só as MAX_NUM_WORDS mais frequentes do corpus\n",
    "tokenizer.fit_on_texts(texts)                  #aplica o modelo nos textos\n",
    "sequences = tokenizer.texts_to_sequences(texts)#cada palavra de cada posição da lista texts vira um token\n",
    "\n",
    "word_index = tokenizer.word_index #word_index =  A dictionary of words and their uniquely assigned integers\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #Pads sequences to the same length. - adiciona 0.0 caso seja menor que o \n",
    "                                                            #tamanho máximo e trunca caso for maior que ele\n",
    "\n",
    "labels = to_categorical(np.asarray(labels)) # to_categorical = Converts a class vector (integers) to binary class matrix. #asarray = converts the input to an array\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0]) #cria um vetor de inteiros do tamanho de data.shape[0]\n",
    "np.random.shuffle(indices) #mistura-se eles aleatoriamente\n",
    "data = data[indices] #atribui os dados e labels de indices randomizados\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) #divide os dados em um fator inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1) #min = Return the smallest item in an iterable or the smallest of two or more arguments.\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) #np.zeros cria uma lista de zeros da dimensão especificada por args\n",
    "for word, i in word_index.items(): #para cada (palavra, numero do token dela) em word_index.itens()\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(256, 4, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(256, 4, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(256, 4, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "#x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(11, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 170 samples, validate on 72 samples\n",
      "Epoch 1/120\n",
      "170/170 [==============================] - 13s 75ms/step - loss: 2.4723 - acc: 0.0706 - val_loss: 2.4334 - val_acc: 0.1250\n",
      "Epoch 2/120\n",
      "170/170 [==============================] - 0s 248us/step - loss: 2.2986 - acc: 0.1706 - val_loss: 2.2143 - val_acc: 0.1806\n",
      "Epoch 3/120\n",
      "170/170 [==============================] - 0s 212us/step - loss: 2.2627 - acc: 0.2235 - val_loss: 2.0665 - val_acc: 0.2083\n",
      "Epoch 4/120\n",
      "170/170 [==============================] - 0s 216us/step - loss: 2.0613 - acc: 0.2824 - val_loss: 2.1018 - val_acc: 0.0972\n",
      "Epoch 5/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 1.9191 - acc: 0.3941 - val_loss: 2.0502 - val_acc: 0.1944\n",
      "Epoch 6/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 1.8385 - acc: 0.3353 - val_loss: 2.4301 - val_acc: 0.1250\n",
      "Epoch 7/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 1.9380 - acc: 0.2471 - val_loss: 2.0626 - val_acc: 0.1806\n",
      "Epoch 8/120\n",
      "170/170 [==============================] - 0s 206us/step - loss: 1.8861 - acc: 0.4118 - val_loss: 2.1044 - val_acc: 0.2222\n",
      "Epoch 9/120\n",
      "170/170 [==============================] - 0s 212us/step - loss: 1.7155 - acc: 0.4353 - val_loss: 2.3077 - val_acc: 0.1806\n",
      "Epoch 10/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 1.7715 - acc: 0.3294 - val_loss: 2.3188 - val_acc: 0.2361\n",
      "Epoch 11/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 1.8506 - acc: 0.3824 - val_loss: 2.2023 - val_acc: 0.1389\n",
      "Epoch 12/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 1.5969 - acc: 0.5059 - val_loss: 2.0591 - val_acc: 0.1944\n",
      "Epoch 13/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 1.4512 - acc: 0.5294 - val_loss: 2.5751 - val_acc: 0.1528\n",
      "Epoch 14/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 1.6171 - acc: 0.3706 - val_loss: 2.2221 - val_acc: 0.1944\n",
      "Epoch 15/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 1.5909 - acc: 0.3882 - val_loss: 2.2161 - val_acc: 0.2222\n",
      "Epoch 16/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 1.3633 - acc: 0.4412 - val_loss: 2.1828 - val_acc: 0.1528\n",
      "Epoch 17/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 1.2216 - acc: 0.6294 - val_loss: 2.1736 - val_acc: 0.2083\n",
      "Epoch 18/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 1.1603 - acc: 0.6059 - val_loss: 2.5219 - val_acc: 0.1528\n",
      "Epoch 19/120\n",
      "170/170 [==============================] - 0s 206us/step - loss: 1.2659 - acc: 0.5588 - val_loss: 2.1490 - val_acc: 0.2083\n",
      "Epoch 20/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 1.1496 - acc: 0.6882 - val_loss: 2.2197 - val_acc: 0.1667\n",
      "Epoch 21/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.9383 - acc: 0.7647 - val_loss: 2.3976 - val_acc: 0.2222\n",
      "Epoch 22/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 1.0151 - acc: 0.6941 - val_loss: 2.7917 - val_acc: 0.1667\n",
      "Epoch 23/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 1.2440 - acc: 0.5412 - val_loss: 2.9230 - val_acc: 0.1667\n",
      "Epoch 24/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 1.4301 - acc: 0.5059 - val_loss: 2.4425 - val_acc: 0.2361\n",
      "Epoch 25/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 1.0302 - acc: 0.7588 - val_loss: 2.2444 - val_acc: 0.1389\n",
      "Epoch 26/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.7205 - acc: 0.8471 - val_loss: 2.1450 - val_acc: 0.2083\n",
      "Epoch 27/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.6377 - acc: 0.8765 - val_loss: 2.4998 - val_acc: 0.1667\n",
      "Epoch 28/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.6210 - acc: 0.9235 - val_loss: 2.3621 - val_acc: 0.1944\n",
      "Epoch 29/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.6710 - acc: 0.8294 - val_loss: 3.0868 - val_acc: 0.1528\n",
      "Epoch 30/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.8143 - acc: 0.7765 - val_loss: 2.2840 - val_acc: 0.2361\n",
      "Epoch 31/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.6748 - acc: 0.8118 - val_loss: 2.4891 - val_acc: 0.1389\n",
      "Epoch 32/120\n",
      "170/170 [==============================] - 0s 218us/step - loss: 0.4723 - acc: 0.9588 - val_loss: 2.2804 - val_acc: 0.2361\n",
      "Epoch 33/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.4386 - acc: 0.9471 - val_loss: 2.7018 - val_acc: 0.1528\n",
      "Epoch 34/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.4388 - acc: 0.9647 - val_loss: 2.5828 - val_acc: 0.2222\n",
      "Epoch 35/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.5037 - acc: 0.8882 - val_loss: 3.0454 - val_acc: 0.1667\n",
      "Epoch 36/120\n",
      "170/170 [==============================] - 0s 218us/step - loss: 0.5416 - acc: 0.8647 - val_loss: 2.7020 - val_acc: 0.2361\n",
      "Epoch 37/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.4845 - acc: 0.9059 - val_loss: 2.6352 - val_acc: 0.1667\n",
      "Epoch 38/120\n",
      "170/170 [==============================] - 0s 212us/step - loss: 0.3719 - acc: 0.9706 - val_loss: 2.4722 - val_acc: 0.2083\n",
      "Epoch 39/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.2746 - acc: 0.9882 - val_loss: 2.3687 - val_acc: 0.1806\n",
      "Epoch 40/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.2352 - acc: 0.9824 - val_loss: 2.4380 - val_acc: 0.2083\n",
      "Epoch 41/120\n",
      "170/170 [==============================] - 0s 165us/step - loss: 0.2116 - acc: 0.9882 - val_loss: 2.3976 - val_acc: 0.1944\n",
      "Epoch 42/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.1959 - acc: 0.9882 - val_loss: 2.4990 - val_acc: 0.2083\n",
      "Epoch 43/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.1839 - acc: 0.9882 - val_loss: 2.4593 - val_acc: 0.1806\n",
      "Epoch 44/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.1746 - acc: 0.9882 - val_loss: 2.6113 - val_acc: 0.1944\n",
      "Epoch 45/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.1697 - acc: 0.9882 - val_loss: 2.6105 - val_acc: 0.2083\n",
      "Epoch 46/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.1715 - acc: 0.9882 - val_loss: 2.9596 - val_acc: 0.2361\n",
      "Epoch 47/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.2186 - acc: 0.9765 - val_loss: 5.4671 - val_acc: 0.1806\n",
      "Epoch 48/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 1.2435 - acc: 0.3529 - val_loss: 4.4793 - val_acc: 0.2083\n",
      "Epoch 49/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 1.8152 - acc: 0.8059 - val_loss: 4.6581 - val_acc: 0.1389\n",
      "Epoch 50/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 1.1995 - acc: 0.7235 - val_loss: 3.1197 - val_acc: 0.1806\n",
      "Epoch 51/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.6010 - acc: 0.8588 - val_loss: 2.6792 - val_acc: 0.1944\n",
      "Epoch 52/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.2298 - acc: 0.9882 - val_loss: 2.5974 - val_acc: 0.2222\n",
      "Epoch 53/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.1906 - acc: 0.9882 - val_loss: 2.5738 - val_acc: 0.2083\n",
      "Epoch 54/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.1720 - acc: 0.9882 - val_loss: 2.5542 - val_acc: 0.2083\n",
      "Epoch 55/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.1587 - acc: 0.9882 - val_loss: 2.5572 - val_acc: 0.2083\n",
      "Epoch 56/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.1480 - acc: 0.9882 - val_loss: 2.5580 - val_acc: 0.2083\n",
      "Epoch 57/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.1387 - acc: 0.9882 - val_loss: 2.5817 - val_acc: 0.2083\n",
      "Epoch 58/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.1305 - acc: 0.9882 - val_loss: 2.5825 - val_acc: 0.2083\n",
      "Epoch 59/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.1231 - acc: 0.9882 - val_loss: 2.6138 - val_acc: 0.2083\n",
      "Epoch 60/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.1163 - acc: 0.9882 - val_loss: 2.5952 - val_acc: 0.1944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.1101 - acc: 0.9882 - val_loss: 2.6523 - val_acc: 0.1944\n",
      "Epoch 62/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.1043 - acc: 0.9882 - val_loss: 2.6137 - val_acc: 0.1944\n",
      "Epoch 63/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0986 - acc: 0.9941 - val_loss: 2.6898 - val_acc: 0.1806\n",
      "Epoch 64/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0931 - acc: 0.9941 - val_loss: 2.6394 - val_acc: 0.1944\n",
      "Epoch 65/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0877 - acc: 0.9941 - val_loss: 2.7560 - val_acc: 0.1944\n",
      "Epoch 66/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0824 - acc: 0.9941 - val_loss: 2.6733 - val_acc: 0.2083\n",
      "Epoch 67/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0769 - acc: 0.9941 - val_loss: 2.8041 - val_acc: 0.1944\n",
      "Epoch 68/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0715 - acc: 0.9941 - val_loss: 2.7184 - val_acc: 0.2083\n",
      "Epoch 69/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0661 - acc: 0.9941 - val_loss: 2.8457 - val_acc: 0.1944\n",
      "Epoch 70/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0608 - acc: 0.9941 - val_loss: 2.7901 - val_acc: 0.2083\n",
      "Epoch 71/120\n",
      "170/170 [==============================] - 0s 165us/step - loss: 0.0558 - acc: 0.9941 - val_loss: 2.9096 - val_acc: 0.1806\n",
      "Epoch 72/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.0513 - acc: 0.9941 - val_loss: 2.8831 - val_acc: 0.2083\n",
      "Epoch 73/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0474 - acc: 0.9941 - val_loss: 2.9971 - val_acc: 0.1944\n",
      "Epoch 74/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0441 - acc: 0.9941 - val_loss: 2.9735 - val_acc: 0.2083\n",
      "Epoch 75/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.0414 - acc: 0.9941 - val_loss: 3.0758 - val_acc: 0.1806\n",
      "Epoch 76/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0391 - acc: 0.9941 - val_loss: 3.0307 - val_acc: 0.2222\n",
      "Epoch 77/120\n",
      "170/170 [==============================] - 0s 212us/step - loss: 0.0372 - acc: 0.9941 - val_loss: 3.1467 - val_acc: 0.1944\n",
      "Epoch 78/120\n",
      "170/170 [==============================] - 0s 206us/step - loss: 0.0355 - acc: 0.9941 - val_loss: 3.0779 - val_acc: 0.2222\n",
      "Epoch 79/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 3.2019 - val_acc: 0.1806\n",
      "Epoch 80/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 3.1179 - val_acc: 0.2222\n",
      "Epoch 81/120\n",
      "170/170 [==============================] - 0s 271us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 3.2459 - val_acc: 0.1806\n",
      "Epoch 82/120\n",
      "170/170 [==============================] - 0s 166us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 3.1659 - val_acc: 0.2222\n",
      "Epoch 83/120\n",
      "170/170 [==============================] - 0s 186us/step - loss: 0.0285 - acc: 0.9941 - val_loss: 3.2736 - val_acc: 0.1806\n",
      "Epoch 84/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0274 - acc: 0.9941 - val_loss: 3.2121 - val_acc: 0.2222\n",
      "Epoch 85/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 3.3130 - val_acc: 0.1806\n",
      "Epoch 86/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 3.2510 - val_acc: 0.2222\n",
      "Epoch 87/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0244 - acc: 0.9941 - val_loss: 3.3555 - val_acc: 0.1806\n",
      "Epoch 88/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0236 - acc: 0.9941 - val_loss: 3.2980 - val_acc: 0.2222\n",
      "Epoch 89/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 3.4076 - val_acc: 0.1806\n",
      "Epoch 90/120\n",
      "170/170 [==============================] - 0s 213us/step - loss: 0.0222 - acc: 0.9941 - val_loss: 3.3470 - val_acc: 0.2222\n",
      "Epoch 91/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0217 - acc: 0.9941 - val_loss: 3.4700 - val_acc: 0.1944\n",
      "Epoch 92/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0212 - acc: 0.9941 - val_loss: 3.3883 - val_acc: 0.2361\n",
      "Epoch 93/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0208 - acc: 0.9941 - val_loss: 3.5261 - val_acc: 0.1944\n",
      "Epoch 94/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0205 - acc: 0.9941 - val_loss: 3.4232 - val_acc: 0.2361\n",
      "Epoch 95/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 3.5642 - val_acc: 0.1806\n",
      "Epoch 96/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0198 - acc: 0.9941 - val_loss: 3.4721 - val_acc: 0.2361\n",
      "Epoch 97/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0196 - acc: 0.9941 - val_loss: 3.5923 - val_acc: 0.1806\n",
      "Epoch 98/120\n",
      "170/170 [==============================] - 0s 242us/step - loss: 0.0193 - acc: 0.9941 - val_loss: 3.5329 - val_acc: 0.2083\n",
      "Epoch 99/120\n",
      "170/170 [==============================] - 0s 201us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 3.6009 - val_acc: 0.1806\n",
      "Epoch 100/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 3.5949 - val_acc: 0.1944\n",
      "Epoch 101/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 3.6175 - val_acc: 0.1806\n",
      "Epoch 102/120\n",
      "170/170 [==============================] - 0s 165us/step - loss: 0.0182 - acc: 0.9941 - val_loss: 3.6511 - val_acc: 0.1944\n",
      "Epoch 103/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.0181 - acc: 0.9941 - val_loss: 3.6403 - val_acc: 0.1944\n",
      "Epoch 104/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0180 - acc: 0.9941 - val_loss: 3.6980 - val_acc: 0.1944\n",
      "Epoch 105/120\n",
      "170/170 [==============================] - 0s 165us/step - loss: 0.0180 - acc: 0.9941 - val_loss: 3.6798 - val_acc: 0.1944\n",
      "Epoch 106/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0178 - acc: 0.9941 - val_loss: 3.7344 - val_acc: 0.1944\n",
      "Epoch 107/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0178 - acc: 0.9941 - val_loss: 3.7133 - val_acc: 0.1806\n",
      "Epoch 108/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0176 - acc: 0.9941 - val_loss: 3.7538 - val_acc: 0.1944\n",
      "Epoch 109/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0177 - acc: 0.9941 - val_loss: 3.7442 - val_acc: 0.1944\n",
      "Epoch 110/120\n",
      "170/170 [==============================] - 0s 183us/step - loss: 0.0174 - acc: 0.9941 - val_loss: 3.7785 - val_acc: 0.1944\n",
      "Epoch 111/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0175 - acc: 0.9941 - val_loss: 3.7743 - val_acc: 0.1806\n",
      "Epoch 112/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0173 - acc: 0.9941 - val_loss: 3.7962 - val_acc: 0.1944\n",
      "Epoch 113/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0173 - acc: 0.9941 - val_loss: 3.8099 - val_acc: 0.1806\n",
      "Epoch 114/120\n",
      "170/170 [==============================] - 0s 195us/step - loss: 0.0172 - acc: 0.9941 - val_loss: 3.8184 - val_acc: 0.1944\n",
      "Epoch 115/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0172 - acc: 0.9941 - val_loss: 3.8407 - val_acc: 0.1806\n",
      "Epoch 116/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0171 - acc: 0.9941 - val_loss: 3.8500 - val_acc: 0.2083\n",
      "Epoch 117/120\n",
      "170/170 [==============================] - 0s 177us/step - loss: 0.0172 - acc: 0.9941 - val_loss: 3.8703 - val_acc: 0.1667\n",
      "Epoch 118/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0170 - acc: 0.9941 - val_loss: 3.8833 - val_acc: 0.2083\n",
      "Epoch 119/120\n",
      "170/170 [==============================] - 0s 189us/step - loss: 0.0171 - acc: 0.9941 - val_loss: 3.9042 - val_acc: 0.1667\n",
      "Epoch 120/120\n",
      "170/170 [==============================] - 0s 171us/step - loss: 0.0169 - acc: 0.9941 - val_loss: 3.9010 - val_acc: 0.2083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1997ebd5ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=500,\n",
    "          epochs=120,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = np.array(predicted)\n",
    "np_predicted = np.zeros_like(aux)\n",
    "np_predicted[np.arange(len(aux)), aux.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizando as notas esperadas e previstas pelo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             expected                           predicted\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"             expected                           predicted\")\n",
    "for count, l in enumerate(predicted, start = 0):\n",
    "    print(str(y_val[count]), np_predicted[count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPEAMENTO REVERSO DAS RESPOSTAS PARA INSPEÇÃO DAS NOTAS DADA PELA REDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resposta = []\n",
    "v = []\n",
    "for val in x_val:\n",
    "    #print(val)\n",
    "    for word in val:\n",
    "        #print(word)\n",
    "        if word in reverse_word_map:\n",
    "            #print(word, reverse_word_map[word])\n",
    "            v.append(reverse_word_map[word])\n",
    "    Resposta.append(v)\n",
    "    v = []\n",
    "\n",
    "Esperado = np.argmax(y_val, axis = 1)\n",
    "Obtido = np.argmax(np_predicted,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Resposta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-42be16a12075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResposta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nota Esperada: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEsperado\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nota Obtida: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mObtido\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Resposta' is not defined"
     ]
    }
   ],
   "source": [
    "print(Resposta[15])\n",
    "print(\"Nota Esperada: \", Esperado[15])\n",
    "print(\"Nota Obtida: \", Obtido[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cientificidade', 'psicologia', 'decorrente', 'multideterminação', 'método', 'forma', 'validação', 'teoria']\n",
      "Nota Esperada:  5\n",
      "Nota Obtida:  0\n"
     ]
    }
   ],
   "source": [
    "print(Resposta[6])\n",
    "print(\"Nota Esperada: \", Esperado[6])\n",
    "print(\"Nota Obtida: \", Obtido[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUADRATIC WEIGHTED KAPPA METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [6 6 6 5 6 6 0 5 4 3 3 5 5 3 3 5 2 5 7 3 5 5 5 2 6 3 5 6 4 3 4 6 5 1 4 8 1\n",
      " 1 3 4 6 4 5 6 5 5 5 1 5 0 6 4 5 0 2 3 3 6 6 4 5 2 6 6 5 6 5 6 4 5 7 9]\n",
      "expected:  [7 4 6 3 7 6 5 3 4 2 3 4 3 3 2 3 2 6 5 5 5 8 6 5 3 2 5 7 6 2 4 4 4 1 4 3 4\n",
      " 0 8 3 6 6 2 6 4 6 4 3 3 8 3 5 2 6 1 4 4 2 4 6 1 6 6 4 5 1 3 8 6 6 6 5]\n",
      "Quadratic Weighted Kappa: 0.16246684350132623\n"
     ]
    }
   ],
   "source": [
    "predicted = np.argmax(np_predicted,axis = 1)\n",
    "expected = np.argmax(y_val, axis = 1)\n",
    "print(\"predicted: \", predicted)\n",
    "print(\"expected: \", expected)\n",
    "qwk = metrics.quadratic_weighted_kappa(predicted, expected) \n",
    "print(\"Quadratic Weighted Kappa:\", qwk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registro de performance\n",
    "\n",
    "#### run 1, qwk: 0.10436893203883502, val_acc = 0.1250  , epochs = 30, vsplit = 0.1\n",
    "#### run 2, qwk: 0.2349468713105075, val_acc = 0.2292  , epochs = 30, vsplit = 0.2\n",
    "#### run 3, qwk: 0.35749318801089935, val_acc = 0.1250  , epochs = 30, vsplit = 0.3\n",
    "#### run 4, qwk: 0.5113365155131265, val_acc = 0.2361  , epochs = 50, vsplit = 0.3\n",
    "####  <font color='red'> run 5, qwk: 0.5361608651571477, val_acc = 0.2500  , epochs = 100, vsplit = 0.3 </font>\n",
    "#### run 5, qwk: 0.46097530711006296, val_acc = 0.1667  , epochs = 120, vsplit = 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre o dataset :\n",
    "\n",
    "#### tamanho de cada resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resposta\n",
      "respondida\n",
      "[20, 12, 17, 17, 10, 27, 24, 6, 21, 15, 20, 15, 15, 14, 36, 12, 25, 34, 25, 13, 17, 4, 11, 31, 27, 17, 15, 23, 18, 21, 10, 10, 37, 15, 25, 13, 34, 10, 21, 29, 22, 29, 22, 23, 11, 26, 18, 13, 34, 17, 33, 17, 28, 18, 22, 25, 19, 28, 22, 21, 13, 20, 40, 32, 16, 14, 31, 26, 38, 37, 43, 19, 19, 8, 21, 38, 27, 28, 8, 26, 18, 46, 23, 9, 33, 35, 13, 31, 20, 17, 14, 11, 30, 12, 25, 8, 29, 17, 26, 32, 21, 22, 27, 26, 10, 5, 23, 20, 12, 26, 16, 15, 24, 15, 12, 21, 34, 9, 11, 16, 18, 21, 17, 29, 15, 21, 23, 16, 31, 18, 13, 11, 35, 21, 17, 18, 19, 21, 10, 16, 25, 16, 24, 8, 22, 20, 16, 19, 10, 15, 20, 11, 8, 23, 22, 14, 21, 14, 17, 22, 22, 22, 17, 25, 18, 22, 22, 21, 23, 22, 1, 23, 22, 7, 22, 18, 22, 23, 22, 23, 22, 22, 23, 21, 14, 20, 5, 22, 21, 12, 17, 11, 21, 22, 17, 21, 20, 20, 24, 1, 24, 21, 20, 19, 24, 23, 17, 21, 19, 23, 20, 17, 22, 12, 20, 14, 25, 20, 20, 26, 21, 8, 22, 20, 9, 22, 14, 15, 25, 23, 19, 10, 3, 22, 20, 21, 17, 23, 21, 25, 22, 16]\n"
     ]
    }
   ],
   "source": [
    "tamanho = []\n",
    "for t in texts:\n",
    "    tamanho.append(len(t.split()))\n",
    "    if(len(t.split()) == 1):\n",
    "        print(t)\n",
    "print(tamanho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tamanho mínimo , tamanho máximo, tamanho médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(tamanho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tamanho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(int(sum(tamanho)/len(tamanho)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
